\documentclass[12pt, a4paper, oneside]{article} % Paper size, default font size and one-sided paper
%\graphicspath{{./Figures/}} % Specifies the directory where pictures are stored
%\usepackage[dcucite]{harvard}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{pdflscape}
\usepackage{rotating}
\usepackage[flushleft]{threeparttable}
\usepackage{multirow}
\usepackage[comma, sort&compress]{natbib}% Use the natbib reference package - read up on this to edit the reference style; if you want text (e.g. Smith et al., 2012) for the in-text references (instead of numbers), remove 'numbers' 
\usepackage{graphicx}
%\bibliographystyle{plainnat}
\bibliographystyle{agsm}
\usepackage[colorlinks = true, citecolor = blue, linkcolor = blue]{hyperref}
%\hypersetup{urlcolor=blue, colorlinks=true} % Colors hyperlinks in blue - change to black if annoying
%\renewcommand[\harvardurl]{URL: \url}
\usepackage{listings}
\usepackage{color}
\definecolor{mygrey}{gray}{0.95}
\lstset{backgroundcolor=\color{mygrey}}
\begin{document}
\title{Machine Learning}
\author{Rob Hayward}
\date{\today}
\maketitle
\subsection*{Introduction}
These are the notes from  \href{https://class.coursera.org/ml-006}{the Peng Machine Learning course}. 

Introduction to the methods and model representations.  This is an example of \emph{supervised learning}.  There is a training set that is used to develop a model.  \lstinline{m} is the number  of training examples, \lstinline{x} are the input variables or features, \lstinline{y} are the output variables \lstinline{(x, y)} is a training example, $(x^{(i)}), 
y^{(i)}$ is a training set.

The aim is to get the parameters of a hypothesis.  The hypothesis is 
\begin{equation}
h_{\theta}(x) = \theta_0 + \theta_1 x
\end{equation}

This is a linear regression model. The aim is to chose the values of the parameters $\theta_0$ and $\theta_1$ to minimise the difference between actual and forecast values. 

Objective function
\begin{equation}
\text{minimise}_{\theta_0, \theta_1)} \text{   for} \frac{1}{2m} \sum (h_0(x) - y)^2
\end{equation}

The cost function
\begin{equation}
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i = 1}^m (h_0 x^{(i)} - y^{(i)})^2
\end{equation}

This is also called the \emph{squared error function}

A simplified version can just look at one of the parameters $(\theta_1)$.  This would be a line through the origin. There are two functions:  $h_{\theta}(x)$ is a function of $x$; $J(\theta_1)$ is a function of $\theta_1$.  

<<cost, fig.height=4, echo=FALSE>>=
par(mfrow = c(1,2))
x <- seq(from = 0, to = 10, by = 2)
y = x
plot(x, y, main = expression(paste(theta[0])), text(6, 2, expression(h[theta](x))))
abline(a = 0, b = 1, col = 'red')
squr.err <- function(y, x, theta){
sum((y - theta * x)^2)/(2*length(x))
}
theta <- -1:3
J <- rep(0, times = length(theta))
for(i in 1:length(theta)){
  J[i] <- squr.err(y = y, x = x, theta = theta[i])
}
plot(theta, J, type = 'l', main = expression(J(theta[1])), xlab = 
       expression(theta[1]), ylab = expression(J(theta[1])))
@
Clearly the value of $\theta_1$ that will minimise the cost function is one.  

Now we have the same case with both papameters changing. This can be shown using \emph{contour plots} or \emph{contour figures}.  Each contour has the same value for the cost function (J).  


\href{http://econometricsense.blogspot.co.uk/2011/11/regression-via-gradient-descent-in-r.html}{Econometric Sense}
<<gradient>>=
x0 <- c(1,1,1,1,1) # column of 1's
x1 <- c(1,2,3,4,5) # original x-values
 
# create the x- matrix of explanatory variables
 
x <- as.matrix(cbind(x0,x1))
 
# create the y-matrix of dependent variables
 
y <- as.matrix(c(3,7,5,11,14))
m <- nrow(y)
 
# implement feature scaling
x.scaled <- x
x.scaled[,2] <- (x[,2] - mean(x[,2]))/sd(x[,2])
 
# analytical results with matrix algebra
 solve(t(x)%*%x)%*%t(x)%*%y # w/o feature scaling
 solve(t(x.scaled)%*%x.scaled)%*%t(x.scaled)%*%y # w/ feature scaling
 
# results using canned lm function match results above
summary(lm(y ~ x[, 2])) # w/o feature scaling
summary(lm(y ~ x.scaled[, 2])) # w/feature scaling
 
# define the gradient function dJ/dtheata: 1/m * (h(x)-y))*x where h(x) = x*theta
# in matrix form this is as follows:
grad <- function(x, y, theta) {
  gradient <- (1/m)* (t(x) %*% ((x %*% t(theta)) - y))
  return(t(gradient))
}
 
# define gradient descent update algorithm
grad.descent <- function(x, maxit){
    theta <- matrix(c(0, 0), nrow=1) # Initialize the parameters
 
    alpha = .05 # set learning rate
    for (i in 1:maxit) {
      theta <- theta - alpha  * grad(x, y, theta)   
    }
 return(theta)
}
 
 
# results without feature scaling
print(grad.descent(x,1000))
 
# results with feature scaling
print(grad.descent(x.scaled,1000))
 
# ----------------------------------------------------------------------- 
# cost and convergence intuition
# -----------------------------------------------------------------------
 
# typically we would iterate the algorithm above until the 
# change in the cost function (as a result of the updated b0 and b1 values)
# was extremely small value 'c'. C would be referred to as the set 'convergence'
# criteria. If C is not met after a given # of iterations, you can increase the
# iterations or change the learning rate 'alpha' to speed up convergence
 
# get results from gradient descent
beta <- grad.descent(x,1000)
 
# define the 'hypothesis function'
h <- function(xi,b0,b1) {
 b0 + b1 * xi 
}
 
# define the cost function   
cost <- t(mat.or.vec(1,m))
  for(i in 1:m) {
    cost[i,1] <-  (1 /(2*m)) * (h(x[i,2],beta[1,1],beta[1,2])- y[i,])^2 
  }
 
totalCost <- colSums(cost)
print(totalCost)
 
# save this as Cost1000
cost1000 <- totalCost
 
# change iterations to 1001 and compute cost1001
beta <- (grad.descent(x,1001))
cost <- t(mat.or.vec(1,m))
  for(i in 1:m) {
    cost[i,1] <-  (1 /(2*m)) * (h(x[i,2],beta[1,1],beta[1,2])- y[i,])^2 
  }
cost1001 <- colSums(cost)
 
# does this difference meet your convergence criteria? 
print(cost1000 - cost1001)
@ 



\end{document}
